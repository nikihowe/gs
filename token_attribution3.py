import torch
from colorama import Fore, Style
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import torch.nn.functional as F
import os

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')

def token_attribution(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    prompt: str,
    completion: str,
) -> list[tuple[str, float]]:
    model.eval()

    # --- 1. Tokenization and Index Identification ---
    full_text = prompt + completion
    # --- FIX: Use rstrip() for boundary calculation ---
    prompt_content_len = len(prompt.rstrip()) # Length excluding trailing whitespace

    print(f"\n--- DEBUG: Inside token_attribution ---")
    print(f"DEBUG: Prompt (first 80 chars): '{prompt[:80]}...'")
    print(f"DEBUG: Completion: '{completion}'")
    print(f"DEBUG: Original Prompt Char Length: {len(prompt)}")
    print(f"DEBUG: Stripped Prompt Boundary Length (for check): {prompt_content_len}")

    try:
        inputs = tokenizer(
            full_text,
            return_tensors='pt',
            truncation=True,
            max_length=getattr(model.config, 'max_position_embeddings', 512),
            return_offsets_mapping=True
        ).to(device)
    except Exception as e:
        print(f"Error during tokenization: {e}")
        return []

    input_ids = inputs['input_ids'][0]
    offsets = inputs['offset_mapping'][0].tolist()
    sequence_length = len(input_ids)

    print(f"DEBUG: Sequence Length (tokens): {sequence_length}")
    # (Optional: Keep offset debug prints if needed)
    # print(f"DEBUG: Offsets (first 10): {offsets[:10]}")
    # print(f"DEBUG: Offsets (last 10): {offsets[-10:]}")

    prompt_start_idx = 0
    if getattr(tokenizer, 'add_bos_token', False) and input_ids[0] == tokenizer.bos_token_id:
        prompt_start_idx = 1
        # (Optional: Keep BOS debug print)
        # print(f"DEBUG: BOS token detected, prompt_start_idx = 1")

    completion_token_start_index = -1
    for idx, (start_char, end_char) in enumerate(offsets):
        if start_char == 0 and end_char == 0:
             if idx == 0 and prompt_start_idx == 1: continue
             else: continue

        # --- FIX: Compare against stripped length ---
        if start_char >= prompt_content_len:
            completion_token_start_index = idx
            print(f"DEBUG: Found completion start at index {idx} using boundary {prompt_content_len}, offset ({start_char}, {end_char})")
            break

    if completion_token_start_index == -1:
         completion_token_start_index = sequence_length
         print(f"DEBUG: Completion start not found via offset, setting to end: {sequence_length}")

    prompt_end_idx = completion_token_start_index
    print(f"DEBUG: Final prompt indices range: [{prompt_start_idx}:{prompt_end_idx}]")
    print(f"DEBUG: Final completion indices range: [{completion_token_start_index}:{sequence_length}]")

    prompt_indices = range(prompt_start_idx, prompt_end_idx)
    completion_indices = range(completion_token_start_index, sequence_length)

    # --- Decode for Debugging (Keep this block) ---
    if sequence_length > 0 :
        actual_prompt_token_ids = input_ids[prompt_indices]
        actual_prompt_tokens_decoded = tokenizer.convert_ids_to_tokens(actual_prompt_token_ids)
        print(f"DEBUG: Tokens assigned to PROMPT: {actual_prompt_tokens_decoded}")

        if completion_indices:
             actual_completion_token_ids = input_ids[completion_indices]
             actual_completion_tokens_decoded = tokenizer.convert_ids_to_tokens(actual_completion_token_ids)
             print(f"DEBUG: Tokens assigned to COMPLETION: {actual_completion_tokens_decoded}")
        else:
             print("DEBUG: Tokens assigned to COMPLETION: [] (Empty Range)")
    else:
         print("DEBUG: Sequence length is 0, cannot decode tokens.")

    # --- Sanity Checks (Keep these, but ensure prompt_tokens list is defined if needed) ---
    if not (0 <= prompt_start_idx <= prompt_end_idx <= completion_token_start_index <= sequence_length):
        print(f"Error: Invalid index calculation. Skipping.")
        print(f"--- END DEBUG (Error): Exiting token_attribution ---\n")
        return []
    # Check if prompt range is valid *before* trying to decode (actual_prompt_tokens_decoded handles empty case now)
    if prompt_start_idx >= prompt_end_idx:
         print("Warning: Prompt has zero tokens.")
         print(f"--- END DEBUG (Warning): Exiting token_attribution ---\n")
         return []
    if not completion_indices:
         print("Warning: Completion has zero tokens. Returning zero attribution.")
         prompt_tokens = actual_prompt_tokens_decoded if 'actual_prompt_tokens_decoded' in locals() else []
         if not prompt_tokens: return []
         print(f"--- END DEBUG (Warning): Exiting token_attribution ---\n")
         return [(token, 0.0) for token in prompt_tokens]


    # --- 2. Forward Pass, Attention Aggregation, Score Calculation ---
    attribution_scores_list = [] # Initialize return list
    try:
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)
            attentions = outputs.attentions

        if attentions is None:
            print("Error: Model did not return attention scores.")
            print(f"--- END DEBUG (Error): Exiting token_attribution ---\n")
            return []

        # --- 3. Attention Aggregation ---
        last_layer_attention = attentions[-1].to(device)
        avg_head_attention = last_layer_attention.mean(dim=1).squeeze(0)

        # --- 4. Calculate Attribution Scores ---
        # (Keep the inner try-except for indexing errors here)
        try:
            attribution_matrix = avg_head_attention[completion_indices, :][:, prompt_indices]
            prompt_token_scores = attribution_matrix.sum(dim=0) # Assign scores here
        except IndexError as e:
             print(f"Error indexing attention matrix: {e}")
             print(f"--- END DEBUG (Error): Exiting token_attribution ---\n")
             return []
        except Exception as e:
             print(f"Error during attribution calculation: {e}")
             print(f"--- END DEBUG (Error): Exiting token_attribution ---\n")
             return []


        # --- 5. Format Output [INSIDE TRY BLOCK] ---
        prompt_token_ids_final = input_ids[prompt_indices]
        prompt_tokens_final = tokenizer.convert_ids_to_tokens(prompt_token_ids_final)
        print(f"DEBUG: Final prompt tokens list being returned: {prompt_tokens_final}")

        # --- FIX: NameError resolved as prompt_token_scores is now defined ---
        if len(prompt_tokens_final) != len(prompt_token_scores):
            print(f"Error: Mismatch final token count ({len(prompt_tokens_final)}) vs score count ({len(prompt_token_scores)}).")
            min_len = min(len(prompt_tokens_final), len(prompt_token_scores))
            attribution_scores_list = list(zip(prompt_tokens_final[:min_len], prompt_token_scores.cpu().tolist()[:min_len]))
        else:
            attribution_scores_list = list(zip(prompt_tokens_final, prompt_token_scores.cpu().tolist()))

    except Exception as e: # Catch errors from forward pass itself or others in the block
        print(f"Error during model forward pass or subsequent processing: {e}")
        print(f"--- END DEBUG (Error): Exiting token_attribution ---\n")
        return [] # Return empty list if anything in the main try block failed


    print(f"--- END DEBUG: Exiting token_attribution ---\n")
    return attribution_scores_list

# --- Rest of your code (visualize_attribution, __main__) below ---


# --------------------------------------------------------------------------
# The rest of your code (visualize_attribution, create_attribution_display,
# and the __main__ block) should remain the same.
# Make sure the __main__ block calls this updated token_attribution function.
# --------------------------------------------------------------------------

import torch # Ensure torch is imported if used for isfinite check
from colorama import Fore, Style, init as colorama_init

# Initialize colorama if not done globally
# colorama_init(autoreset=True)

def visualize_attribution(
    tokens_with_attributions: list[tuple[str, float]],
    max_attr: float,
) -> str:
    """
    Visualizes attribution scores using a 5-color scale.

    Args:
        tokens_with_attributions: List of (token_string, score) tuples.
        max_attr: The maximum attribution score for normalization.

    Returns:
        A string with tokens colored based on their normalized scores.
    """
    colored_text = ''
    # Use a small epsilon to avoid division by zero and handle very small max_attr
    if max_attr <= 1e-9:
        print(f"DEBUG: max_attr ({max_attr:.4f}) is very small or non-positive, using 1.0 for scaling.")
        max_attr = 1.0

    # --- Define Thresholds for 5 Colors (Green < Cyan < Yellow < Magenta < Red) ---
    # IMPORTANT: Adjust these thresholds based on your raw score distribution!
    # These examples are tuned low based on previous debug output (max~50, others <1.0).
    threshold_cyan    = 0.005 # Scores > 0.005 (0.5% of max) up to 0.010 are Cyan
    threshold_yellow  = 0.010 # Scores > 0.010 (1.0% of max) up to 0.015 are Yellow
    threshold_magenta = 0.015 # Scores > 0.015 (1.5% of max) up to 0.500 are Magenta
    threshold_red     = 0.500 # Scores > 0.500 (50% of max) are Red (Should catch 'Human')
                               # Scores <= 0.005 are Green

    # --- Alternatively, use evenly spaced thresholds if scores are distributed differently ---
    # threshold_cyan = 0.20
    # threshold_yellow = 0.40
    # threshold_magenta = 0.60
    # threshold_red = 0.80
    # ---

    # Debug print to show thresholds being used
    print(f"DEBUG: Visualizing with max_attr={max_attr:.4f}, Thresh: G<={threshold_cyan:.4f}<C<={threshold_yellow:.4f}<Y<={threshold_magenta:.4f}<M<={threshold_red:.4f}<R")

    for token, attribution in tokens_with_attributions:
        # Ensure score is valid before processing
        if not isinstance(attribution, (float, int)) or not torch.isfinite(torch.tensor(attribution)):
            attribution = 0.0 # Default to 0 for NaN, infinity, or non-numeric types

        # Normalize score relative to max_attr (clamped between 0 and 1)
        norm_attribution = max(0.0, attribution) / max_attr
        norm_attribution = min(norm_attribution, 1.0) # Clamp top end just in case

        # Determine color using the defined thresholds
        if norm_attribution > threshold_red:
            color = Fore.RED      # Highest importance
        elif norm_attribution > threshold_magenta:
            color = Fore.MAGENTA  # High-Medium importance
        elif norm_attribution > threshold_yellow:
            color = Fore.YELLOW   # Medium importance
        elif norm_attribution > threshold_cyan:
            color = Fore.CYAN     # Low-Medium importance
        else:
            color = Fore.GREEN    # Lowest importance

        # Handle token display string (handling spaces, bytes)
        display_token = token
        if isinstance(token, str) and token.startswith('Ä '):
            display_token = ' ' + token[1:]
        elif isinstance(token, bytes):
             display_token = token.decode('utf-8', errors='replace')

        # Append colored token to the result string
        colored_text += f'{color}{display_token}{Style.RESET_ALL}'

    return colored_text

# (Include create_attribution_display function here if needed, although the main block bypasses it)
def create_attribution_display(model, tokenizer):
     # ... (same as before) ...
     pass # Placeholder if not pasting the whole thing

# ==============================================================================
# Main Execution Block (Copy-paste from here down)
# ==============================================================================
if __name__ == '__main__':

    # --- Configuration ---
    # Base Model Checkpoint
    # checkpoint = 'HuggingFaceTB/SmolLM2-135M-Instruct'
    checkpoint = 'HuggingFaceTB/SmolLM2-1.7B-Instruct' # Ensure this model exists

    # Path to your fine-tuned DPO model directory
    trained_model_path = 'big_dpo_model' # Ensure this path is correct relative to script execution

    # Path to your dataset
    dataset_path = './datasets/dataset.json' # Ensure path is correct

    # Generation parameters
    gen_max_length = 150  # Max length for generated responses
    num_examples_to_show = 5 # Number of examples to process from the dataset

    # --- Load Models and Tokenizer ---
    print(f"Loading tokenizer from: {checkpoint}")
    try:
        # Trust remote code if necessary for certain model architectures (use with caution)
        tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)

        print(f"Loading base model from: {checkpoint}")
        base_model = AutoModelForCausalLM.from_pretrained(
            checkpoint,
            trust_remote_code=True,
            attn_implementation="eager"  # Use eager attention for attention score output
        ).to(device)

        print(f"Loading tuned model from: {trained_model_path}")
        if not os.path.isdir(trained_model_path):
             raise FileNotFoundError(f"Tuned model directory not found: {trained_model_path}")
        trained_model = AutoModelForCausalLM.from_pretrained(
            trained_model_path,
            trust_remote_code=True,
            attn_implementation="eager"  # Use eager attention here too
        ).to(device)

    except Exception as e:
        print(f"\nError loading model/tokenizer: {e}")
        print("Please ensure checkpoints/paths are correct, you have dependencies installed,")
        print("and network connectivity if downloading for the first time.")
        exit(1) # Exit if models can't load

    # --- Tokenizer Pad Token Handling ---
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"Set tokenizer pad_token to eos_token ({tokenizer.eos_token})")
        else:
            # Add a fallback pad token if EOS is also missing (less ideal)
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            print("Added '[PAD]' as pad_token.")
        # Important: Resize model embeddings if new tokens were added
        base_model.resize_token_embeddings(len(tokenizer))
        trained_model.resize_token_embeddings(len(tokenizer))
        # Assign pad_token_id to model configs
        base_model.config.pad_token_id = tokenizer.pad_token_id
        trained_model.config.pad_token_id = tokenizer.pad_token_id


    # --- Set Models to Evaluation Mode ---
    base_model.eval()
    trained_model.eval()
    print("Models set to evaluation mode.")

    # --- Load Dataset ---
    print(f"Loading dataset from: {dataset_path}")
    try:
        with open(dataset_path, 'r', encoding='utf-8') as fp: # Added encoding
            dataset = json.load(fp)
    except FileNotFoundError:
        print(f"Error: Dataset file not found at {dataset_path}")
        exit(1)
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from {dataset_path}. Details: {e}")
        exit(1)
    except Exception as e:
        print(f"An unexpected error occurred loading the dataset: {e}")
        exit(1)

    # Validate dataset structure
    if not isinstance(dataset, dict) or 'text' not in dataset or 'harmful' not in dataset:
         print("Error: Dataset JSON must be an object with 'text' and 'harmful' keys.")
         exit(1)
    if not isinstance(dataset['text'], list) or not isinstance(dataset['harmful'], list):
         print("Error: Dataset 'text' and 'harmful' values must be lists.")
         exit(1)
    if len(dataset['text']) != len(dataset['harmful']):
         print(f"Error: Dataset 'text' ({len(dataset['text'])}) and 'harmful' ({len(dataset['harmful'])}) lists must have the same length.")
         exit(1)

    print(f"Loaded dataset with {len(dataset['text'])} examples.")

    # --- Prepare Examples ---
    good_examples = [
        text for text, label in zip(dataset['text'], dataset['harmful']) if isinstance(label, (int, float)) and label == 0
    ]
    # Add standard prompt formatting
    good_examples_prompts = ['Human: ' + str(text) + ' Assistant: ' for text in good_examples] # Added str() for safety

    num_available_good_examples = len(good_examples_prompts)
    print(f"Found {num_available_good_examples} 'good' examples.")

    if num_available_good_examples == 0:
        print("No 'good' examples (label=0) found in the dataset to process.")
        exit(0)

    # Adjust number of examples to show if fewer are available
    num_examples_to_show = min(num_examples_to_show, num_available_good_examples)

    print(f"\nProcessing {num_examples_to_show} examples...")
    print("-" * 60)

    # --- Main Processing Loop ---
    for i in range(num_examples_to_show):
        current_prompt = good_examples_prompts[i]
        print(f"\n--- Example {i+1} / {num_examples_to_show} ---")
        print(f"Prompt: {current_prompt[:300]}{'...' if len(current_prompt)>300 else ''}") # Truncate display

        base_model_completion = "" # Initialize completions
        tuned_model_completion = ""

        # --- Generate Actual Completions ---
        print("\nGenerating base model response...")
        try:
            inputs_tokenized = tokenizer(current_prompt, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length).to(device) # Truncate prompt if too long
            # Generation arguments
            gen_args = {
                 "max_length": len(inputs_tokenized['input_ids'][0]) + gen_max_length, # Generate up to gen_max_length *new* tokens
                 "num_return_sequences": 1,
                 "eos_token_id": tokenizer.eos_token_id,
                 "pad_token_id": tokenizer.pad_token_id,
                 "do_sample": False, # Use greedy decoding for reproducibility, or True for sampling
                 # "temperature": 0.7, # Example if do_sample=True
                 # "top_p": 0.9,      # Example if do_sample=True
                 "no_repeat_ngram_size": 3 # Reduce repetition
            }

            with torch.no_grad(): # Ensure no gradients are calculated during generation
                 base_model_output = base_model.generate(
                     input_ids=inputs_tokenized.input_ids,
                     attention_mask=inputs_tokenized.attention_mask,
                     **gen_args
                 )

            # Decode only the generated part
            base_model_generated_ids = base_model_output[0, inputs_tokenized.input_ids.shape[1]:] # Slice generated IDs
            base_model_completion = tokenizer.decode(base_model_generated_ids, skip_special_tokens=True).strip()
            print(f"Base Model Generated: '{base_model_completion[:150]}...'")

        except Exception as e:
            print(f"Error during base model generation: {e}")
            base_model_completion = "[GENERATION ERROR]" # Indicate error

        print("Generating tuned model response...")
        try:
            inputs_tokenized = tokenizer(current_prompt, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length).to(device)
            gen_args['max_length'] = len(inputs_tokenized['input_ids'][0]) + gen_max_length # Recalculate max_length based on potentially truncated input

            with torch.no_grad():
                tuned_model_output = trained_model.generate(
                    input_ids=inputs_tokenized.input_ids,
                    attention_mask=inputs_tokenized.attention_mask,
                    **gen_args
                )

            tuned_model_generated_ids = tuned_model_output[0, inputs_tokenized.input_ids.shape[1]:]
            tuned_model_completion = tokenizer.decode(tuned_model_generated_ids, skip_special_tokens=True).strip()
            print(f"Tuned Model Generated: '{tuned_model_completion[:150]}...'")

        except Exception as e:
            print(f"Error during tuned model generation: {e}")
            tuned_model_completion = "[GENERATION ERROR]" # Indicate error


        # --- Calculate Attributions with ACTUAL Generated Completion ---
        base_model_good_attribution = [] # Initialize as empty lists
        tuned_model_good_attribution = []

        if base_model_completion and base_model_completion != "[GENERATION ERROR]":
            print(f"\nCalculating attribution for BASE model...")
            base_model_good_attribution = token_attribution(
                base_model, tokenizer, current_prompt, base_model_completion
            )
        else:
            print("\nSkipping base model attribution due to empty/failed generation.")

        if tuned_model_completion and tuned_model_completion != "[GENERATION ERROR]":
            print(f"Calculating attribution for TUNED model...")
            tuned_model_good_attribution = token_attribution(
                trained_model, tokenizer, current_prompt, tuned_model_completion
            )
        else:
             print("Skipping tuned model attribution due to empty/failed generation.")


        # --- Visualize Attributions ---
        print("\n--- Attributions ---")
        if base_model_good_attribution:
             # Filter out potential NaN/inf scores before finding max
             valid_scores = [attr for _, attr in base_model_good_attribution if isinstance(attr, (int, float)) and torch.isfinite(torch.tensor(attr))]
             max_base_attr = max(valid_scores) if valid_scores else 0.0
             # Print raw scores (optional - print first few)
             print(f"DEBUG: Raw Base Attribution Scores (first 10): {base_model_good_attribution[:10]}")
             print(
                'Base model:',
                visualize_attribution(base_model_good_attribution, max_attr=max_base_attr), # Colored prompt
                base_model_completion # Show the actual completion used
             )
        else:
            print("Base model: [Attribution not calculated]")

        if tuned_model_good_attribution:
             valid_scores = [attr for _, attr in tuned_model_good_attribution if isinstance(attr, (int, float)) and torch.isfinite(torch.tensor(attr))]
             max_tuned_attr = max(valid_scores) if valid_scores else 0.0
             # Print raw scores (optional - print first few)
             print(f"DEBUG: Raw Tuned Attribution Scores (first 10): {tuned_model_good_attribution[:10]}")
             print(
                 'Tuned model:',
                 visualize_attribution(tuned_model_good_attribution, max_attr=max_tuned_attr), # Colored prompt
                 tuned_model_completion # Show the actual completion used
             )
        else:
             print("Tuned model: [Attribution not calculated]")

        print("-" * 60) # Separator between examples

    print("\nProcessing finished.")